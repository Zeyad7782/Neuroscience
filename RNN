import numpy as np
words = ["Liverpool", "is", "wonderful", "club"]
vocab = list(set(words))
word_to_idx = {w: i for i, w in enumerate(vocab)}
idx_to_word = {i: w for w, i in word_to_idx.items()}
vocab_size = len(vocab)
def one_hot(idx, size):
    vec = np.zeros((size,))
    vec[idx] = 1
    return vec
X = [one_hot(word_to_idx[w], vocab_size) for w in ["Liverpool", "is", "wonderful"]]
Y = word_to_idx["club"]
input_size = vocab_size
hidden_size = 8
output_size = vocab_size
Wxh = np.random.randn(hidden_size, input_size) * 0.01  
Whh = np.random.randn(hidden_size, hidden_size) * 0.01  
Why = np.random.randn(output_size, hidden_size) * 0.01  
bh = np.zeros((hidden_size,))
by = np.zeros((output_size,))
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

learning_rate = 0.1
for epoch in range(1000):
    h_prev = np.zeros((hidden_size,))
    hs = []
    
    for x_t in X:
        h_prev = np.tanh(np.dot(Wxh, x_t) + np.dot(Whh, h_prev) + bh)
        hs.append(h_prev)
    y_pred = np.dot(Why, h_prev) + by
    probs = softmax(y_pred)
    loss = -np.log(probs[Y])
    dWhy = probs.copy()
    dWhy[Y] -= 1 
    dWhy = np.outer(dWhy, hs[-1])
    Why -= learning_rate * dWhy
    by -= learning_rate * (probs.copy() - one_hot(Y, vocab_size))
    if (epoch + 1) % 100 == 0:
        pred_idx = np.argmax(probs)
        print(f"Epoch {epoch+1} | Loss: {loss:.4f} | Prediction: {idx_to_word[pred_idx]}")
